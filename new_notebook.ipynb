{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4D Projection Pipeline\n",
    "\n",
    "Self-supervised occupancy prediction using photometric consistency on TartanDrive 2.0.\n",
    "\n",
    "Pipeline: RGB to stereo depth to backproject to 3D to transform via TartanVO pose to reproject to photometric loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a1dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "  PyTorch 2.3.1, CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import yaml\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"  PyTorch {torch.__version__}, CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95b4a4",
   "metadata": {},
   "source": [
    "## 1. Configuration & Path Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f47a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paths ────────────────────────────────────────────────────────────────\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "EXTRACTED_DIR = BASE_DIR / \"extracted\"\n",
    "\n",
    "IMAGE_DIR = EXTRACTED_DIR / \"images\" / \"multisense_left_image_rect_color\"\n",
    "ODOM_DIR  = EXTRACTED_DIR / \"odometry\"\n",
    "CALIB_DIR = EXTRACTED_DIR / \"calibration\"\n",
    "DEBUG_DIR = EXTRACTED_DIR / \"projection_debug\"\n",
    "\n",
    "DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ecd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Training hyperparameters ─────────────────────────────────────────────\n",
    "LEARNING_RATE  = 1e-4\n",
    "NUM_EPOCHS     = 3\n",
    "BATCH_SIZE     = 2\n",
    "IMG_HEIGHT     = 544\n",
    "IMG_WIDTH      = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739e9b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Base dir:   /Users/maxmagnusson/Documents/Master Thesis UGV Route Planning/Code/VTUGV\n",
      "  Image dir:  /Users/maxmagnusson/Documents/Master Thesis UGV Route Planning/Code/VTUGV/extracted/images/multisense_left_image_rect_color\n",
      "  Odom dir:   /Users/maxmagnusson/Documents/Master Thesis UGV Route Planning/Code/VTUGV/extracted/odometry\n"
     ]
    }
   ],
   "source": [
    "# ── Voxel grid parameters (for future QueryOcc) ─────────────────────────\n",
    "VOXEL_X_RANGE  = (-10.0, 10.0)   # meters, lateral\n",
    "VOXEL_Y_RANGE  = (-3.0,  3.0)    # meters, vertical\n",
    "VOXEL_Z_RANGE  = (0.5,   50.0)   # meters, forward (depth)\n",
    "VOXEL_SIZE     = 0.5             # meters per voxel\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Base dir:   {BASE_DIR}\")\n",
    "print(f\"  Image dir:  {IMAGE_DIR}\")\n",
    "print(f\"  Odom dir:   {ODOM_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf73c8",
   "metadata": {},
   "source": [
    "## 2. Data Calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca72cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_intrinsics(filepath):\n",
    "    \"\"\"\n",
    "    Parse multisense_intrinsics.txt to extract the camera intrinsic matrix K\n",
    "    and projection matrix P for the left color camera.\n",
    "\n",
    "    Returns:\n",
    "        K:  (3, 3) intrinsic matrix\n",
    "        P:  (3, 4) projection matrix\n",
    "        D:  distortion coefficients\n",
    "        img_size: (width, height)\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # We want the left_image_rect_color section\n",
    "    sections = content.split(\"---\")\n",
    "    target_section = None\n",
    "    for section in sections:\n",
    "        if \"left_image_rect_color\" in section or \"left/camera_info\" in section:\n",
    "            # Prefer left_image_rect_color if present\n",
    "            if \"left_image_rect_color\" in section:\n",
    "                target_section = section\n",
    "                break\n",
    "            elif target_section is None:\n",
    "                target_section = section\n",
    "\n",
    "    if target_section is None:\n",
    "        target_section = sections[0]  # fallback to first section\n",
    "\n",
    "    # Parse K matrix (3x3 stored as flat list of 9)\n",
    "    for line in target_section.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"K:\"):\n",
    "            k_str = line[2:].strip()\n",
    "            k_vals = [float(x) for x in k_str.strip(\"[]\").split(\",\")]\n",
    "            K = np.array(k_vals).reshape(3, 3)\n",
    "        elif line.startswith(\"P:\"):\n",
    "            p_str = line[2:].strip()\n",
    "            p_vals = [float(x) for x in p_str.strip(\"[]\").split(\",\")]\n",
    "            P = np.array(p_vals).reshape(3, 4)\n",
    "        elif line.startswith(\"D:\"):\n",
    "            d_str = line[2:].strip()\n",
    "            d_vals = [float(x) for x in d_str.strip(\"[]\").split(\",\")]\n",
    "            D = np.array(d_vals)\n",
    "        elif line.startswith(\"width:\"):\n",
    "            width = int(line.split(\":\")[1].strip())\n",
    "        elif line.startswith(\"height:\"):\n",
    "            height = int(line.split(\":\")[1].strip())\n",
    "\n",
    "    return K, P, D, (width, height)\n",
    "\n",
    "\n",
    "def parse_extrinsics(filepath):\n",
    "    \"\"\"\n",
    "    Parse extrinsics.yaml to build transformation matrices.\n",
    "\n",
    "    Returns dict of transform name -> 4x4 homogeneous matrix.\n",
    "    Key transforms:\n",
    "      - 'vehicle_to_multisense_head':  T from vehicle frame to camera head\n",
    "      - 'multisense_head_to_left_optical': T from head to left camera optical frame\n",
    "      - 'vehicle_to_left_optical': composed full chain\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    transforms = {}\n",
    "    for entry in data[\"transform_params\"]:\n",
    "        t = np.array(entry[\"translation\"])\n",
    "        q = entry[\"quaternion\"]  # [x, y, z, w]\n",
    "        R = Rotation.from_quat(q).as_matrix()  # scipy uses [x,y,z,w]\n",
    "\n",
    "        T = np.eye(4)\n",
    "        T[:3, :3] = R\n",
    "        T[:3, 3] = t\n",
    "\n",
    "        key = f\"{entry['from_frame']}_to_{entry['to_frame']}\"\n",
    "        # Clean up slashes in frame names\n",
    "        key = key.replace(\"/\", \"_\").replace(\"multisense_\", \"ms_\")\n",
    "        transforms[key] = T\n",
    "\n",
    "    # Compose vehicle → left_camera_optical_frame\n",
    "    # Chain: vehicle → multisense/head → multisense/left_camera_optical_frame\n",
    "    T_v2head = None\n",
    "    T_head2optical = None\n",
    "\n",
    "    for entry in data[\"transform_params\"]:\n",
    "        if entry[\"from_frame\"] == \"vehicle\" and \"head\" in entry[\"to_frame\"]:\n",
    "            t = np.array(entry[\"translation\"])\n",
    "            q = entry[\"quaternion\"]\n",
    "            R = Rotation.from_quat(q).as_matrix()\n",
    "            T_v2head = np.eye(4)\n",
    "            T_v2head[:3, :3] = R\n",
    "            T_v2head[:3, 3] = t\n",
    "\n",
    "        if \"head\" in entry[\"from_frame\"] and \"left_camera_optical\" in entry[\"to_frame\"]:\n",
    "            t = np.array(entry[\"translation\"])\n",
    "            q = entry[\"quaternion\"]\n",
    "            R = Rotation.from_quat(q).as_matrix()\n",
    "            T_head2optical = np.eye(4)\n",
    "            T_head2optical[:3, :3] = R\n",
    "            T_head2optical[:3, 3] = t\n",
    "\n",
    "    if T_v2head is not None and T_head2optical is not None:\n",
    "        T_vehicle_to_cam = T_head2optical @ T_v2head\n",
    "        transforms[\"vehicle_to_left_optical\"] = T_vehicle_to_cam\n",
    "    else:\n",
    "        print(\"  WARNING: Could not compose full vehicle→camera chain\")\n",
    "        # Identity fallback\n",
    "        transforms[\"vehicle_to_left_optical\"] = np.eye(4)\n",
    "\n",
    "    return transforms\n",
    "\n",
    "\n",
    "def parse_odometry(filepath):\n",
    "    \"\"\"\n",
    "    Parse TartanVO odometry CSV.\n",
    "\n",
    "    Returns:\n",
    "        timestamps: list of float (seconds.nanoseconds)\n",
    "        poses: list of 4x4 numpy arrays (vehicle-to-world)\n",
    "    \"\"\"\n",
    "    timestamps = []\n",
    "    poses = []\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ts = float(row[\"timestamp_sec\"]) + float(row[\"timestamp_nsec\"]) * 1e-9\n",
    "            timestamps.append(ts)\n",
    "\n",
    "            pos = np.array([\n",
    "                float(row[\"pos_x\"]),\n",
    "                float(row[\"pos_y\"]),\n",
    "                float(row[\"pos_z\"])\n",
    "            ])\n",
    "            quat = np.array([\n",
    "                float(row[\"orient_x\"]),\n",
    "                float(row[\"orient_y\"]),\n",
    "                float(row[\"orient_z\"]),\n",
    "                float(row[\"orient_w\"])\n",
    "            ])\n",
    "\n",
    "            R = Rotation.from_quat(quat).as_matrix()  # [x,y,z,w] format\n",
    "            T = np.eye(4)\n",
    "            T[:3, :3] = R\n",
    "            T[:3, 3] = pos\n",
    "\n",
    "            poses.append(T)\n",
    "\n",
    "    return np.array(timestamps), poses\n",
    "\n",
    "\n",
    "def parse_image_timestamps(filepath):\n",
    "    \"\"\"\n",
    "    Parse the image timestamps CSV.\n",
    "\n",
    "    Returns:\n",
    "        frame_indices: list of int\n",
    "        timestamps: list of float (seconds.nanoseconds)\n",
    "        filenames: list of str\n",
    "    \"\"\"\n",
    "    frame_indices = []\n",
    "    timestamps = []\n",
    "    filenames = []\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            frame_indices.append(int(row[\"frame_index\"]))\n",
    "            ts = float(row[\"timestamp_sec\"]) + float(row[\"timestamp_nsec\"]) * 1e-9\n",
    "            timestamps.append(ts)\n",
    "            filenames.append(row[\"filename\"])\n",
    "\n",
    "    return frame_indices, np.array(timestamps), filenames\n",
    "\n",
    "\n",
    "def match_poses_to_frames(image_timestamps, odom_timestamps, odom_poses):\n",
    "    \"\"\"\n",
    "    For each image timestamp, find the nearest odometry pose.\n",
    "\n",
    "    Returns:\n",
    "        matched_poses: list of 4x4 arrays (one per image frame)\n",
    "        time_offsets: list of float (time difference in seconds)\n",
    "    \"\"\"\n",
    "    matched_poses = []\n",
    "    time_offsets = []\n",
    "\n",
    "    for img_ts in image_timestamps:\n",
    "        diffs = np.abs(odom_timestamps - img_ts)\n",
    "        best_idx = np.argmin(diffs)\n",
    "        matched_poses.append(odom_poses[best_idx])\n",
    "        time_offsets.append(float(diffs[best_idx]))\n",
    "\n",
    "    return matched_poses, time_offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c7d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Section 2: Data Calibration ──\n",
      "  Intrinsics K:\n",
      "[[455.77496338   0.         497.11801147]\n",
      " [  0.         456.3190918  251.58502197]\n",
      " [  0.           0.           1.        ]]\n",
      "  Image size: (0, 0)\n",
      "  T_vehicle_to_cam (shape (4, 4)):\n",
      "  [[-0.22474233 -0.02357802  0.97413293 -0.131     ]\n",
      " [-0.97441597  0.00328796 -0.22472805  0.1999    ]\n",
      " [ 0.00209573 -0.99971659 -0.02371374 -0.0641    ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "  Loaded 94 odometry poses\n",
      "  Odom time range: 1694708261.987 → 1694708274.306s\n",
      "  Loaded 104 image timestamps\n",
      "  Pose matching: mean offset=51.2ms, max=506.2ms\n"
     ]
    }
   ],
   "source": [
    "# ── Run calibration ──────────────────────────────────────────────────────\n",
    "print(\"\\n── Section 2: Data Calibration ──\")\n",
    "\n",
    "K, P, D, img_size = parse_intrinsics(CALIB_DIR / \"multisense_intrinsics.txt\")\n",
    "print(f\"  Intrinsics K:\\n{K}\")\n",
    "print(f\"  Image size: {img_size}\")\n",
    "\n",
    "extrinsics = parse_extrinsics(CALIB_DIR / \"extrinsics.yaml\")\n",
    "T_vehicle_to_cam = extrinsics[\"vehicle_to_left_optical\"]\n",
    "T_cam_to_vehicle = np.linalg.inv(T_vehicle_to_cam)\n",
    "print(f\"  T_vehicle_to_cam (shape {T_vehicle_to_cam.shape}):\")\n",
    "print(f\"  {T_vehicle_to_cam}\")\n",
    "\n",
    "odom_timestamps, odom_poses = parse_odometry(ODOM_DIR / \"tartanvo_odom.csv\")\n",
    "print(f\"  Loaded {len(odom_poses)} odometry poses\")\n",
    "print(f\"  Odom time range: {odom_timestamps[0]:.3f} → {odom_timestamps[-1]:.3f}s\")\n",
    "\n",
    "frame_indices, img_timestamps, img_filenames = parse_image_timestamps(\n",
    "    IMAGE_DIR / \"timestamps.csv\"\n",
    ")\n",
    "print(f\"  Loaded {len(frame_indices)} image timestamps\")\n",
    "\n",
    "matched_poses, time_offsets = match_poses_to_frames(\n",
    "    img_timestamps, odom_timestamps, odom_poses\n",
    ")\n",
    "max_offset = max(time_offsets)\n",
    "mean_offset = np.mean(time_offsets)\n",
    "print(f\"  Pose matching: mean offset={mean_offset*1000:.1f}ms, max={max_offset*1000:.1f}ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2801888",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a667c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TartanDrive4DDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset returning temporal triplets (T-1, T, T+1) with poses.\n",
    "\n",
    "    Each sample provides everything needed for the self-supervised\n",
    "    photometric consistency forward pass:\n",
    "      - Three consecutive RGB frames (prev, curr, next)\n",
    "      - 4x4 world poses for all three frames\n",
    "      - Camera intrinsic matrix K\n",
    "\n",
    "    No ground-truth depth is needed — the DepthNet predicts it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, frame_indices, filenames,\n",
    "                 matched_poses, img_timestamps, K,\n",
    "                 img_height=IMG_HEIGHT, img_width=IMG_WIDTH):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "\n",
    "        # K as torch tensor (3x3)\n",
    "        self.K = torch.from_numpy(K.astype(np.float32))\n",
    "        self.K_inv = torch.inverse(self.K)\n",
    "\n",
    "        # Build valid triplets: need frames at index i-1, i, i+1\n",
    "        self.triplets = []\n",
    "        for i in range(1, len(frame_indices) - 1):\n",
    "            # Check that timestamps are reasonably close (reject large gaps)\n",
    "            dt_prev = img_timestamps[i] - img_timestamps[i - 1]\n",
    "            dt_next = img_timestamps[i + 1] - img_timestamps[i]\n",
    "            if dt_prev > 0.5 or dt_next > 0.5:  # skip if gap > 500ms\n",
    "                continue\n",
    "\n",
    "            self.triplets.append({\n",
    "                \"prev_img\": str(self.image_dir / filenames[i - 1]),\n",
    "                \"curr_img\": str(self.image_dir / filenames[i]),\n",
    "                \"next_img\": str(self.image_dir / filenames[i + 1]),\n",
    "                \"pose_prev\": matched_poses[i - 1],\n",
    "                \"pose_curr\": matched_poses[i],\n",
    "                \"pose_next\": matched_poses[i + 1],\n",
    "                \"timestamp\": img_timestamps[i],\n",
    "                \"frame_idx\": frame_indices[i],\n",
    "            })\n",
    "\n",
    "        print(f\"  TartanDrive4DDataset: {len(self.triplets)} valid triplets \"\n",
    "              f\"from {len(frame_indices)} frames\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.triplets[idx]\n",
    "\n",
    "        # Load images (BGR → RGB, normalize to [0, 1])\n",
    "        img_prev = cv2.imread(sample[\"prev_img\"])\n",
    "        img_curr = cv2.imread(sample[\"curr_img\"])\n",
    "        img_next = cv2.imread(sample[\"next_img\"])\n",
    "\n",
    "        img_prev = cv2.cvtColor(img_prev, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        img_curr = cv2.cvtColor(img_curr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        img_next = cv2.cvtColor(img_next, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "        # (H, W, 3) → (3, H, W)\n",
    "        img_prev = torch.from_numpy(img_prev).permute(2, 0, 1)\n",
    "        img_curr = torch.from_numpy(img_curr).permute(2, 0, 1)\n",
    "        img_next = torch.from_numpy(img_next).permute(2, 0, 1)\n",
    "\n",
    "        # Poses as 4x4 tensors\n",
    "        pose_prev = torch.from_numpy(sample[\"pose_prev\"].astype(np.float32))\n",
    "        pose_curr = torch.from_numpy(sample[\"pose_curr\"].astype(np.float32))\n",
    "        pose_next = torch.from_numpy(sample[\"pose_next\"].astype(np.float32))\n",
    "\n",
    "        return {\n",
    "            \"img_prev\": img_prev,\n",
    "            \"img_curr\": img_curr,\n",
    "            \"img_next\": img_next,\n",
    "            \"pose_prev\": pose_prev,\n",
    "            \"pose_curr\": pose_curr,\n",
    "            \"pose_next\": pose_next,\n",
    "            \"K\":        self.K,\n",
    "            \"K_inv\":    self.K_inv,\n",
    "            \"timestamp\": sample[\"timestamp\"],\n",
    "            \"frame_idx\": sample[\"frame_idx\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c026861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Section 3: Dataset & DataLoader ──\n",
      "  TartanDrive4DDataset: 100 valid triplets from 104 frames\n",
      "  Sample shapes:\n",
      "    img_curr:  torch.Size([3, 544, 1024])\n",
      "    pose_curr: torch.Size([4, 4])\n",
      "    K:         torch.Size([3, 3])\n",
      "    timestamp: 1694708261.577674\n"
     ]
    }
   ],
   "source": [
    "# ── Create dataset and dataloader ────────────────────────────────────────\n",
    "print(\"\\n── Section 3: Dataset & DataLoader ──\")\n",
    "\n",
    "dataset = TartanDrive4DDataset(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    frame_indices=frame_indices,\n",
    "    filenames=img_filenames,\n",
    "    matched_poses=matched_poses,\n",
    "    img_timestamps=img_timestamps,\n",
    "    K=K,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Quick sanity check\n",
    "sample = dataset[0]\n",
    "print(f\"  Sample shapes:\")\n",
    "print(f\"    img_curr:  {sample['img_curr'].shape}\")\n",
    "print(f\"    pose_curr: {sample['pose_curr'].shape}\")\n",
    "print(f\"    K:         {sample['K'].shape}\")\n",
    "print(f\"    timestamp: {sample['timestamp']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ae201",
   "metadata": {},
   "source": [
    "## 4. Projection (the 4D pipeline core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "362f03c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Section 4: Projection Functions Defined ──\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  The 4D projection pipeline:\n",
    "#\n",
    "#    P_world = T_v2w · T_c2v · (D · K⁻¹ · [u, v, 1]ᵀ)\n",
    "#\n",
    "#  For photometric consistency we don't need to go all the way to world\n",
    "#  coordinates. Instead we compute the RELATIVE transform between two\n",
    "#  camera frames and warp one image into the other's viewpoint:\n",
    "#\n",
    "#    1. Backproject current depth map to 3D points in camera frame\n",
    "#    2. Transform 3D points from current camera to neighbor camera\n",
    "#    3. Project transformed points into the neighbor's image plane\n",
    "#    4. Sample the neighbor image at the projected coordinates\n",
    "#    5. Compare sampled image with actual current image → loss\n",
    "#\n",
    "\n",
    "\n",
    "def backproject_depth_to_3d(depth, K_inv):\n",
    "    \"\"\"\n",
    "    Backproject a depth map into 3D points in the camera coordinate frame.\n",
    "\n",
    "    Args:\n",
    "        depth: (B, 1, H, W) depth map in meters\n",
    "        K_inv: (B, 3, 3)    inverse intrinsic matrix\n",
    "\n",
    "    Returns:\n",
    "        points_3d: (B, 3, H*W) 3D points in camera frame\n",
    "    \"\"\"\n",
    "    B, _, H, W = depth.shape\n",
    "\n",
    "    # Create pixel coordinate grid\n",
    "    v, u = torch.meshgrid(\n",
    "        torch.arange(H, dtype=torch.float32, device=depth.device),\n",
    "        torch.arange(W, dtype=torch.float32, device=depth.device),\n",
    "        indexing=\"ij\"\n",
    "    )\n",
    "    ones = torch.ones_like(u)\n",
    "\n",
    "    # Homogeneous pixel coordinates: (3, H*W)\n",
    "    pixel_coords = torch.stack([u, v, ones], dim=0).reshape(3, -1)  # (3, H*W)\n",
    "    pixel_coords = pixel_coords.unsqueeze(0).expand(B, -1, -1)       # (B, 3, H*W)\n",
    "\n",
    "    # Normalized camera rays: K⁻¹ · [u, v, 1]ᵀ\n",
    "    cam_rays = K_inv @ pixel_coords  # (B, 3, H*W)\n",
    "\n",
    "    # Scale by depth: D · K⁻¹ · [u, v, 1]ᵀ\n",
    "    depth_flat = depth.reshape(B, 1, H * W)  # (B, 1, H*W)\n",
    "    points_3d = cam_rays * depth_flat         # (B, 3, H*W)\n",
    "\n",
    "    return points_3d\n",
    "\n",
    "\n",
    "def transform_points_3d(points_3d, T):\n",
    "    \"\"\"\n",
    "    Apply a 4×4 rigid transform to 3D points.\n",
    "\n",
    "    Args:\n",
    "        points_3d: (B, 3, N)  3D points\n",
    "        T:         (B, 4, 4)  transformation matrix\n",
    "\n",
    "    Returns:\n",
    "        transformed: (B, 3, N) transformed 3D points\n",
    "    \"\"\"\n",
    "    B, _, N = points_3d.shape\n",
    "    R = T[:, :3, :3]  # (B, 3, 3)\n",
    "    t = T[:, :3, 3:]  # (B, 3, 1)\n",
    "\n",
    "    transformed = R @ points_3d + t  # (B, 3, N)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def project_3d_to_2d(points_3d, K):\n",
    "    \"\"\"\n",
    "    Project 3D points onto the image plane.\n",
    "\n",
    "    Args:\n",
    "        points_3d: (B, 3, N) 3D points in camera frame\n",
    "        K:         (B, 3, 3) intrinsic matrix\n",
    "\n",
    "    Returns:\n",
    "        pixel_coords: (B, 2, N) pixel coordinates (u, v)\n",
    "        valid_mask:    (B, N)    True where depth > 0 (in front of camera)\n",
    "    \"\"\"\n",
    "    # Project: K · P_3d → [u*z, v*z, z]\n",
    "    projected = K @ points_3d  # (B, 3, N)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    z = projected[:, 2:3, :]  # (B, 1, N)\n",
    "    z = z.clamp(min=1e-6)\n",
    "\n",
    "    pixel_coords = projected[:, :2, :] / z  # (B, 2, N)\n",
    "\n",
    "    # Valid if z > 0 (point is in front of the camera)\n",
    "    valid_mask = (points_3d[:, 2, :] > 0.1)  # (B, N)\n",
    "\n",
    "    return pixel_coords, valid_mask\n",
    "\n",
    "\n",
    "def compute_relative_transform(T_src, T_tgt, T_cam_to_vehicle_torch, T_vehicle_to_cam_torch):\n",
    "    \"\"\"\n",
    "    Compute the relative camera-to-camera transform between two frames.\n",
    "\n",
    "    The full chain:\n",
    "      P_cam_tgt = T_v2c · T_world2v_tgt · T_v2world_src · T_c2v · P_cam_src\n",
    "\n",
    "    Simplified: T_rel = T_v2c · inv(T_tgt) · T_src · T_c2v\n",
    "\n",
    "    Args:\n",
    "        T_src: (B, 4, 4) source frame world pose (vehicle-to-world)\n",
    "        T_tgt: (B, 4, 4) target frame world pose (vehicle-to-world)\n",
    "        T_cam_to_vehicle_torch: (4, 4) static camera-to-vehicle transform\n",
    "        T_vehicle_to_cam_torch: (4, 4) static vehicle-to-camera transform\n",
    "\n",
    "    Returns:\n",
    "        T_rel: (B, 4, 4) relative transform: src_cam → tgt_cam\n",
    "    \"\"\"\n",
    "    B = T_src.shape[0]\n",
    "\n",
    "    T_c2v = T_cam_to_vehicle_torch.unsqueeze(0).expand(B, -1, -1)  # (B, 4, 4)\n",
    "    T_v2c = T_vehicle_to_cam_torch.unsqueeze(0).expand(B, -1, -1)  # (B, 4, 4)\n",
    "\n",
    "    # inv(T_tgt): world-to-vehicle of target\n",
    "    T_tgt_inv = torch.inverse(T_tgt)\n",
    "\n",
    "    # Relative transform: src_cam → src_vehicle → world → tgt_vehicle → tgt_cam\n",
    "    T_rel = T_v2c @ T_tgt_inv @ T_src @ T_c2v\n",
    "\n",
    "    return T_rel\n",
    "\n",
    "\n",
    "def warp_image(src_img, tgt_depth, K, K_inv, T_tgt_to_src):\n",
    "    \"\"\"\n",
    "    Warp a source image into the target camera's viewpoint using the\n",
    "    target's depth map and the relative camera transform.\n",
    "\n",
    "    This is the core of photometric consistency:\n",
    "      1. Backproject target depth → 3D points in target camera frame\n",
    "      2. Transform to source camera frame via T_tgt_to_src\n",
    "      3. Project onto source image plane\n",
    "      4. Sample source image at projected coordinates\n",
    "\n",
    "    Args:\n",
    "        src_img:       (B, 3, H, W)  source RGB image\n",
    "        tgt_depth:     (B, 1, H, W)  target depth map\n",
    "        K:             (B, 3, 3)     intrinsic matrix\n",
    "        K_inv:         (B, 3, 3)     inverse intrinsic matrix\n",
    "        T_tgt_to_src:  (B, 4, 4)     transform from target cam to source cam\n",
    "\n",
    "    Returns:\n",
    "        warped_img:    (B, 3, H, W)  source image warped to target viewpoint\n",
    "        valid_mask:    (B, 1, H, W)  mask of valid projections\n",
    "    \"\"\"\n",
    "    B, _, H, W = src_img.shape\n",
    "\n",
    "    # Step 1: Backproject target depth to 3D\n",
    "    pts_3d = backproject_depth_to_3d(tgt_depth, K_inv)  # (B, 3, H*W)\n",
    "\n",
    "    # Step 2: Transform to source camera frame\n",
    "    pts_in_src = transform_points_3d(pts_3d, T_tgt_to_src)  # (B, 3, H*W)\n",
    "\n",
    "    # Step 3: Project onto source image plane\n",
    "    pixel_coords, valid = project_3d_to_2d(pts_in_src, K)  # (B, 2, H*W), (B, H*W)\n",
    "\n",
    "    # Step 4: Normalize to [-1, 1] for grid_sample\n",
    "    u = pixel_coords[:, 0, :]  # (B, H*W)\n",
    "    v = pixel_coords[:, 1, :]  # (B, H*W)\n",
    "\n",
    "    u_norm = 2.0 * u / (W - 1) - 1.0\n",
    "    v_norm = 2.0 * v / (H - 1) - 1.0\n",
    "\n",
    "    # Also mark out-of-bounds pixels as invalid\n",
    "    valid = valid & (u_norm > -1) & (u_norm < 1) & (v_norm > -1) & (v_norm < 1)\n",
    "\n",
    "    # Reshape for grid_sample: (B, H, W, 2)\n",
    "    grid = torch.stack([u_norm, v_norm], dim=-1).reshape(B, H, W, 2)\n",
    "\n",
    "    # Step 5: Sample source image\n",
    "    warped_img = F.grid_sample(\n",
    "        src_img, grid,\n",
    "        mode=\"bilinear\",\n",
    "        padding_mode=\"zeros\",\n",
    "        align_corners=True\n",
    "    )\n",
    "\n",
    "    valid_mask = valid.reshape(B, 1, H, W).float()\n",
    "\n",
    "    # Also mask out pixels with zero depth (no valid depth)\n",
    "    depth_valid = (tgt_depth > 0).float()\n",
    "    valid_mask = valid_mask * depth_valid\n",
    "\n",
    "    return warped_img, valid_mask\n",
    "\n",
    "\n",
    "print(\"\\n── Section 4: Projection Functions Defined ──\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "933de584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Convert static transforms to torch ───────────────────────────────────\n",
    "T_cam_to_vehicle_torch = torch.from_numpy(T_cam_to_vehicle.astype(np.float32)).to(DEVICE)\n",
    "T_vehicle_to_cam_torch = torch.from_numpy(T_vehicle_to_cam.astype(np.float32)).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7aaeec",
   "metadata": {},
   "source": [
    "## 5. Loss Functions & Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30da153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(x, y, window_size=3):\n",
    "    \"\"\"\n",
    "    Compute the Structural Similarity Index (SSIM) between two images.\n",
    "\n",
    "    Args:\n",
    "        x, y: (B, 3, H, W) images in [0, 1]\n",
    "        window_size: size of the averaging window\n",
    "\n",
    "    Returns:\n",
    "        ssim_map: (B, 1, H, W) per-pixel SSIM (1 = identical)\n",
    "    \"\"\"\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    pad = window_size // 2\n",
    "\n",
    "    mu_x = F.avg_pool2d(x, window_size, stride=1, padding=pad)\n",
    "    mu_y = F.avg_pool2d(y, window_size, stride=1, padding=pad)\n",
    "\n",
    "    mu_x_sq = mu_x ** 2\n",
    "    mu_y_sq = mu_y ** 2\n",
    "    mu_xy = mu_x * mu_y\n",
    "\n",
    "    sigma_x_sq = F.avg_pool2d(x ** 2, window_size, stride=1, padding=pad) - mu_x_sq\n",
    "    sigma_y_sq = F.avg_pool2d(y ** 2, window_size, stride=1, padding=pad) - mu_y_sq\n",
    "    sigma_xy = F.avg_pool2d(x * y, window_size, stride=1, padding=pad) - mu_xy\n",
    "\n",
    "    ssim_num = (2 * mu_xy + C1) * (2 * sigma_xy + C2)\n",
    "    ssim_den = (mu_x_sq + mu_y_sq + C1) * (sigma_x_sq + sigma_y_sq + C2)\n",
    "\n",
    "    ssim_map = ssim_num / ssim_den\n",
    "    return ssim_map.mean(dim=1, keepdim=True)  # average over RGB → (B, 1, H, W)\n",
    "\n",
    "\n",
    "def photometric_loss(pred_img, target_img, valid_mask, alpha=0.85):\n",
    "    \"\"\"\n",
    "    Combined photometric reconstruction loss: α·SSIM + (1-α)·L1.\n",
    "\n",
    "    Args:\n",
    "        pred_img:    (B, 3, H, W) warped/reconstructed image\n",
    "        target_img:  (B, 3, H, W) ground truth image\n",
    "        valid_mask:  (B, 1, H, W) valid pixel mask\n",
    "        alpha:       weight for SSIM vs L1\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar photometric loss\n",
    "    \"\"\"\n",
    "    # L1 loss\n",
    "    l1 = (pred_img - target_img).abs().mean(dim=1, keepdim=True)  # (B, 1, H, W)\n",
    "\n",
    "    # SSIM loss (1 - SSIM, so lower is better)\n",
    "    ssim_val = ssim(pred_img, target_img)\n",
    "    ssim_loss = (1.0 - ssim_val) / 2.0  # scale to [0, 1]\n",
    "\n",
    "    # Combined\n",
    "    combined = alpha * ssim_loss + (1.0 - alpha) * l1\n",
    "\n",
    "    # Apply mask: only count valid pixels\n",
    "    masked = combined * valid_mask\n",
    "    if valid_mask.sum() > 0:\n",
    "        loss = masked.sum() / valid_mask.sum()\n",
    "    else:\n",
    "        loss = masked.sum()  # will be 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def smoothness_loss(depth, image):\n",
    "    \"\"\"\n",
    "    Edge-aware depth smoothness loss.\n",
    "    Depth gradients are penalized less at image edges (likely object boundaries).\n",
    "\n",
    "    Args:\n",
    "        depth: (B, 1, H, W) depth or inverse depth\n",
    "        image: (B, 3, H, W) RGB image\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar smoothness loss\n",
    "    \"\"\"\n",
    "    # Depth gradients\n",
    "    grad_depth_x = torch.abs(depth[:, :, :, :-1] - depth[:, :, :, 1:])\n",
    "    grad_depth_y = torch.abs(depth[:, :, :-1, :] - depth[:, :, 1:, :])\n",
    "\n",
    "    # Image gradients (for edge-awareness)\n",
    "    grad_img_x = torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:]).mean(dim=1, keepdim=True)\n",
    "    grad_img_y = torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]).mean(dim=1, keepdim=True)\n",
    "\n",
    "    # Weight depth smoothness by exp(-image_gradient)\n",
    "    grad_depth_x *= torch.exp(-grad_img_x)\n",
    "    grad_depth_y *= torch.exp(-grad_img_y)\n",
    "\n",
    "    return grad_depth_x.mean() + grad_depth_y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "632c9aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Section 5: Loss Functions & Training ──\n"
     ]
    }
   ],
   "source": [
    "# ── Training Loop ────────────────────────────────────────────────────────\n",
    "print(\"\\n── Section 5: Loss Functions & Training ──\")\n",
    "\n",
    "\n",
    "class DepthNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-supervised monocular depth prediction network.\n",
    "\n",
    "    A U-Net-style encoder-decoder that takes a single RGB image and\n",
    "    predicts a dense depth map. Trained via photometric consistency\n",
    "    loss — no ground-truth depth required.\n",
    "\n",
    "    Architecture: 3-level U-Net with skip connections.\n",
    "    Output activation: Softplus (guarantees positive depth).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, 3, padding=1),\n",
    "            nn.Softplus(),  # ensures positive depth output\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: (B, 3, H, W) RGB image\n",
    "\n",
    "        Returns:\n",
    "            depth: (B, 1, H, W) predicted depth map\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        e1 = self.enc1(rgb)   # (B, 32, H, W)\n",
    "        e2 = self.enc2(e1)    # (B, 64, H/2, W/2)\n",
    "        e3 = self.enc3(e2)    # (B, 128, H/4, W/4)\n",
    "\n",
    "        # Decode with skip connections\n",
    "        d3 = self.dec3(e3)    # (B, 64, H/2, W/2)\n",
    "        d2 = self.dec2(torch.cat([d3, e2], dim=1))  # (B, 32, H, W)\n",
    "        d1 = self.dec1(torch.cat([d2, e1], dim=1))  # (B, 1, H, W)\n",
    "\n",
    "        return d1\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device,\n",
    "                    T_c2v, T_v2c, smooth_weight=0.001):\n",
    "    \"\"\"\n",
    "    Train for one epoch using photometric consistency loss.\n",
    "\n",
    "    For each triplet (prev, curr, next):\n",
    "      1. Predict depth for the current frame from RGB alone\n",
    "      2. Warp previous frame to current viewpoint using predicted depth\n",
    "      3. Warp next frame to current viewpoint using predicted depth\n",
    "      4. Compute photometric loss between warped and actual current frame\n",
    "      5. Add depth smoothness regularization\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        img_prev = batch[\"img_prev\"].to(device)\n",
    "        img_curr = batch[\"img_curr\"].to(device)\n",
    "        img_next = batch[\"img_next\"].to(device)\n",
    "        pose_prev = batch[\"pose_prev\"].to(device)\n",
    "        pose_curr = batch[\"pose_curr\"].to(device)\n",
    "        pose_next = batch[\"pose_next\"].to(device)\n",
    "        K_batch   = batch[\"K\"].to(device)\n",
    "        K_inv_batch = batch[\"K_inv\"].to(device)\n",
    "\n",
    "        # Forward pass: predict depth from RGB\n",
    "        depth_pred = model(img_curr)\n",
    "\n",
    "        # ── Warp previous → current ──\n",
    "        T_curr_to_prev = compute_relative_transform(\n",
    "            pose_curr, pose_prev, T_c2v, T_v2c\n",
    "        )\n",
    "        warped_prev, mask_prev = warp_image(\n",
    "            img_prev, depth_pred, K_batch, K_inv_batch, T_curr_to_prev\n",
    "        )\n",
    "\n",
    "        # ── Warp next → current ──\n",
    "        T_curr_to_next = compute_relative_transform(\n",
    "            pose_curr, pose_next, T_c2v, T_v2c\n",
    "        )\n",
    "        warped_next, mask_next = warp_image(\n",
    "            img_next, depth_pred, K_batch, K_inv_batch, T_curr_to_next\n",
    "        )\n",
    "\n",
    "        # ── Photometric losses ──\n",
    "        loss_prev = photometric_loss(warped_prev, img_curr, mask_prev)\n",
    "        loss_next = photometric_loss(warped_next, img_curr, mask_next)\n",
    "\n",
    "        # Take minimum of forward/backward (handles occlusions)\n",
    "        photo_loss = torch.min(loss_prev, loss_next)\n",
    "\n",
    "        # ── Smoothness loss on predicted depth ──\n",
    "        smooth_loss = smoothness_loss(depth_pred, img_curr)\n",
    "\n",
    "        # ── Total loss ──\n",
    "        loss = photo_loss + smooth_weight * smooth_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "    return total_loss / max(count, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757f370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model: DepthNet (543,489 parameters)\n",
      "  Optimizer: Adam, lr=0.0001\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ── Instantiate model and optimizer ──────────────────────────────────────\n",
    "model = DepthNet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  Model: DepthNet ({num_params:,} parameters)\")\n",
    "print(f\"  Optimizer: Adam, lr={LEARNING_RATE}\")\n",
    "print(f\"  Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "054ae7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Running projection sanity check...\n",
      "  ✓ Saved projection debug image: /Users/maxmagnusson/Documents/Master Thesis UGV Route Planning/Code/VTUGV/extracted/projection_debug/projection_sanity_check.png\n",
      "    Valid pixels: 201441 / 557056\n",
      "    Predicted depth range: 0.68 – 0.70 m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DepthNet(\n",
       "  (enc1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (enc2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (enc3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec3): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec2): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec1): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Softplus(beta=1.0, threshold=20.0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── Sanity check: warp using predicted depth ─────────────────────────────\n",
    "print(\"\\n  Running projection sanity check...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = dataset[5] if len(dataset) > 5 else dataset[0]\n",
    "    img_src = sample[\"img_prev\"].unsqueeze(0).to(DEVICE)   # (1, 3, H, W)\n",
    "    img_tgt = sample[\"img_curr\"].unsqueeze(0).to(DEVICE)   # (1, 3, H, W)\n",
    "    K_b     = sample[\"K\"].unsqueeze(0).to(DEVICE)           # (1, 3, 3)\n",
    "    K_inv_b = sample[\"K_inv\"].unsqueeze(0).to(DEVICE)       # (1, 3, 3)\n",
    "    pose_src = sample[\"pose_prev\"].unsqueeze(0).to(DEVICE)  # (1, 4, 4)\n",
    "    pose_tgt = sample[\"pose_curr\"].unsqueeze(0).to(DEVICE)  # (1, 4, 4)\n",
    "\n",
    "    # Predict depth from current RGB\n",
    "    depth = model(img_tgt)  # (1, 1, H, W)\n",
    "\n",
    "    # Relative transform: target_cam → source_cam\n",
    "    T_tgt_to_src = compute_relative_transform(\n",
    "        pose_tgt, pose_src,\n",
    "        T_cam_to_vehicle_torch, T_vehicle_to_cam_torch\n",
    "    )\n",
    "\n",
    "    warped, mask = warp_image(img_src, depth, K_b, K_inv_b, T_tgt_to_src)\n",
    "\n",
    "# Save debug visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "axes[0, 0].imshow(img_tgt[0].cpu().permute(1, 2, 0).numpy())\n",
    "axes[0, 0].set_title(\"Target frame (current)\")\n",
    "axes[0, 1].imshow(img_src[0].cpu().permute(1, 2, 0).numpy())\n",
    "axes[0, 1].set_title(\"Source frame (previous)\")\n",
    "axes[1, 0].imshow(warped[0].cpu().detach().permute(1, 2, 0).numpy().clip(0, 1))\n",
    "axes[1, 0].set_title(\"Warped source → target viewpoint\")\n",
    "axes[1, 1].imshow(depth[0, 0].cpu().detach().numpy(), cmap=\"turbo\")\n",
    "axes[1, 1].set_title(\"Predicted depth (before training)\")\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "debug_path = DEBUG_DIR / \"projection_sanity_check.png\"\n",
    "plt.savefig(debug_path, dpi=100, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"  ✓ Saved projection debug image: {debug_path}\")\n",
    "print(f\"    Valid pixels: {mask.sum().item():.0f} / {mask.numel()}\")\n",
    "print(f\"    Predicted depth range: {depth.min().item():.2f} – {depth.max().item():.2f} m\")\n",
    "\n",
    "model.train()  # back to training mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c8101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Starting training for 10 epochs...\n",
      "  Epoch   1/10 — loss: 0.357668\n",
      "  Epoch   2/10 — loss: 0.313827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m losses = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     epoch_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mT_cam_to_vehicle_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_vehicle_to_cam_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     losses.append(epoch_loss)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m — loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 128\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device, T_c2v, T_v2c, smooth_weight)\u001b[39m\n\u001b[32m    125\u001b[39m loss = photo_loss + smooth_weight * smooth_loss\n\u001b[32m    127\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m optimizer.step()\n\u001b[32m    131\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/dml/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    517\u001b[39m         Tensor.backward,\n\u001b[32m    518\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    523\u001b[39m         inputs=inputs,\n\u001b[32m    524\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/dml/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    262\u001b[39m     retain_graph = create_graph\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/dml/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ── Training loop ────────────────────────────────────────────────────────\n",
    "print(f\"\\n  Starting training for {NUM_EPOCHS} epochs...\")\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = train_one_epoch(\n",
    "        model, dataloader, optimizer, DEVICE,\n",
    "        T_cam_to_vehicle_torch, T_vehicle_to_cam_torch,\n",
    "    )\n",
    "    losses.append(epoch_loss)\n",
    "    print(f\"  Epoch {epoch+1:3d}/{NUM_EPOCHS} — loss: {epoch_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fdb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Plot loss curve ──────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(losses) + 1), losses, \"b-o\", linewidth=2, markersize=4)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Photometric Loss\")\n",
    "plt.title(\"Self-Supervised Training — Photometric Consistency Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "loss_plot_path = DEBUG_DIR / \"training_loss.png\"\n",
    "plt.savefig(loss_plot_path, dpi=100)\n",
    "plt.close()\n",
    "print(f\"\\n  ✓ Loss curve saved: {loss_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35392ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Final visualization: predicted depth & warping ────────────────────────\n",
    "print(\"\\n  Generating final visualization...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = dataset[len(dataset) // 2]\n",
    "    img_curr = sample[\"img_curr\"].unsqueeze(0).to(DEVICE)\n",
    "    img_prev = sample[\"img_prev\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Predict depth from RGB\n",
    "    depth_pred = model(img_curr)\n",
    "\n",
    "    K_b = sample[\"K\"].unsqueeze(0).to(DEVICE)\n",
    "    K_inv_b = sample[\"K_inv\"].unsqueeze(0).to(DEVICE)\n",
    "    pose_prev_b = sample[\"pose_prev\"].unsqueeze(0).to(DEVICE)\n",
    "    pose_curr_b = sample[\"pose_curr\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    T_rel = compute_relative_transform(\n",
    "        pose_curr_b, pose_prev_b,\n",
    "        T_cam_to_vehicle_torch, T_vehicle_to_cam_torch\n",
    "    )\n",
    "\n",
    "    warped, mask = warp_image(img_prev, depth_pred, K_b, K_inv_b, T_rel)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(24, 10))\n",
    "\n",
    "    axes[0, 0].imshow(img_curr[0].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[0, 0].set_title(\"Target (current frame)\", fontsize=14)\n",
    "    axes[0, 1].imshow(img_prev[0].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[0, 1].set_title(\"Source (previous frame)\", fontsize=14)\n",
    "    axes[0, 2].imshow(warped[0].cpu().permute(1, 2, 0).numpy().clip(0, 1))\n",
    "    axes[0, 2].set_title(\"Warped source → target viewpoint\", fontsize=14)\n",
    "\n",
    "    axes[1, 0].imshow(depth_pred[0, 0].cpu().numpy(), cmap=\"turbo\")\n",
    "    axes[1, 0].set_title(\"Predicted depth (after training)\", fontsize=14)\n",
    "    axes[1, 1].imshow(mask[0, 0].cpu().numpy(), cmap=\"gray\")\n",
    "    axes[1, 1].set_title(\"Valid projection mask\", fontsize=14)\n",
    "    diff = (warped[0] - img_curr[0]).abs().mean(dim=0).cpu().numpy()\n",
    "    axes[1, 2].imshow(diff, cmap=\"hot\", vmin=0, vmax=0.3)\n",
    "    axes[1, 2].set_title(\"Photometric error\", fontsize=14)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(\"Self-Supervised Depth Prediction — Results\", fontsize=16, y=1.01)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    final_path = DEBUG_DIR / \"final_results.png\"\n",
    "    plt.savefig(final_path, dpi=100, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"  ✓ Final results saved: {final_path}\")\n",
    "    print(f\"    Predicted depth range: {depth_pred.min().item():.2f} – {depth_pred.max().item():.2f} m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save model checkpoint ────────────────────────────────────────────────\n",
    "checkpoint_path = BASE_DIR / \"depth_net_checkpoint.pth\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"losses\": losses,\n",
    "    \"epoch\": NUM_EPOCHS,\n",
    "}, checkpoint_path)\n",
    "print(f\"  ✓ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"  Pipeline complete!\")\n",
    "print(f\"  Debug outputs: {DEBUG_DIR}\")\n",
    "print(f\"  Checkpoint:    {checkpoint_path}\")\n",
    "print(\"=\" * 72)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
